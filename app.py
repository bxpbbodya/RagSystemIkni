# app.py
from __future__ import annotations

from pathlib import Path
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
import json
import zipfile

import streamlit as st
import pandas as pd

# matplotlib safe backend (important for Windows / streamlit)
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from core.config import CONFIG
from core.index import load_faiss_index, build_faiss_index, load_chunks_from_jsonl
from core.security import mask_secret
from core.llm import LLMSettings, chat_completion, build_base_url

# ‚úÖ NEW structured answer API
from core.rag import (
    make_answer_no_llm_struct,
    make_answer_with_llm_struct,
)

# Optional reranker
try:
    from core.rerank import rerank_results
    RERANK_AVAILABLE = True
except Exception:
    rerank_results = None
    RERANK_AVAILABLE = False

FEEDBACK_PATH = Path("data/feedback.jsonl")


# ==========================================================
# Page config
# ==========================================================
st.set_page_config(page_title=CONFIG.project_name, layout="wide")


# ==========================================================
# Safe JSONL helpers (DO NOT CRASH on broken lines)
# ==========================================================
def safe_read_jsonl(path: Path, limit: Optional[int] = None) -> List[Dict[str, Any]]:
    if not path.exists():
        return []

    rows: List[Dict[str, Any]] = []
    try:
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = (line or "").strip()
                if not line:
                    continue
                try:
                    rows.append(json.loads(line))
                except Exception:
                    # skip broken line
                    continue
    except Exception:
        return []

    if limit is not None and limit > 0:
        return rows[-limit:]
    return rows


def safe_append_jsonl(path: Path, item: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        with path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    except Exception:
        pass


# ==========================================================
# Feedback
# ==========================================================
def _append_feedback(entry: dict) -> None:
    safe_append_jsonl(FEEDBACK_PATH, entry)


def _feedback_payload(rating: int, comment: str = "") -> dict:
    query = st.session_state.get("last_query", "")
    ans = st.session_state.get("last_answer_struct")
    results = st.session_state.get("last_results", [])

    answer_text = ""
    used_sources: List[int] = []
    warnings: List[str] = []

    if ans:
        answer_text = (getattr(ans, "markdown", "") or "")[:1200]
        used_sources = list(getattr(ans, "used_sources", []) or [])
        warnings = list(getattr(ans, "warnings", []) or [])

    sources = []
    for rank, (chunk, score) in enumerate(results[:10], start=1):
        extra = chunk.extra or {}
        sources.append({
            "rank": rank,
            "score": float(score),
            "title": chunk.title,
            "url": chunk.url,
            "source_type": chunk.source_type,
            "date": chunk.date,
            "chunk_id": chunk.chunk_id,
            "doc_id": extra.get("doc_id"),
        })

    payload = {
        "ts": datetime.now().isoformat(timespec="seconds"),
        "rating": int(rating),
        "comment": (comment or "").strip(),
        "query": query,
        "answer_snippet": answer_text,
        "used_sources": used_sources,
        "warnings": warnings,
        "retrieval_sources": sources,
        "online_mode": bool(st.session_state.get("online_mode")),
        "llm_enabled": bool(st.session_state.get("llm_enabled")),
        "llm_provider": st.session_state.get("llm_provider"),
        "llm_model": st.session_state.get("llm_model"),
        "reranker_enabled": bool(st.session_state.get("use_reranker_ui")),
        "min_score": float(st.session_state.get("min_score", 0.0)),
        "keyword_filter": bool(st.session_state.get("keyword_filter", True)),
        "doc_scope_enabled": bool(st.session_state.get("doc_scope_enabled", False)),
        "doc_scope_ids": sorted(list(st.session_state.get("doc_scope_ids", set()) or [])),
    }
    return payload


# ==========================================================
# Model presets
# ==========================================================
MODEL_PRESETS = {
    "openai": [
        "gpt-4o-mini",
        "gpt-4o",
        "gpt-4.1-mini",
        "gpt-4.1",
    ],
    "groq": [
        "llama-3.1-8b-instant",
        "llama-3.1-70b-versatile",
        "llama3-70b-8192",
        "mixtral-8x7b-32768",
        "gemma2-9b-it",
    ],
    "openrouter": [
        "meta-llama/llama-3.1-70b-instruct",
        "meta-llama/llama-3.1-8b-instruct",
        "google/gemini-2.0-flash-exp",
        "anthropic/claude-3.5-sonnet",
    ],
    "ollama": [
        "llama3.1",
        "mistral",
        "qwen2.5",
    ],
    "custom": [],
}
PROVIDERS = ["openai", "groq", "openrouter", "ollama", "custom"]


# ==========================================================
# Session state
# ==========================================================
def _init_state():
    defaults = {
        # Online mode
        "online_mode": True,

        # RAG state
        "index_ready": False,
        "last_results": [],
        "last_answer_struct": None,
        "last_query": "",
        "last_sync_report": None,

        # LLM
        "llm_enabled": False,
        "llm_provider": "openai",
        "llm_model": "gpt-4o-mini",
        "llm_api_key": "",
        "llm_base_url": "",
        "llm_temperature": 0.2,
        "llm_debug": False,
        "use_custom_model": False,

        # Retrieval tuning
        "min_score": 0.35,
        "keyword_filter": True,
        "show_retrieval_debug": False,

        # Reranker
        "use_reranker_ui": bool(getattr(CONFIG, "use_reranker", False)) and RERANK_AVAILABLE,
        "reranker_model_ui": getattr(CONFIG, "reranker_model_name", "cross-encoder/ms-marco-MiniLM-L-6-v2"),
        "reranker_top_n_ui": int(getattr(CONFIG, "reranker_top_n", 30)),

        # Source filters
        "filter_lpnu": True,
        "filter_tg": True,
        "filter_vns": True,
        "filter_local": True,

        # Document scope (local uploads)
        "doc_scope_enabled": False,
        "doc_scope_ids": set(),

        # Answer UI
        "show_used_sources_only": False,
        "show_chunk_preview": True,

        # quick query
        "quick_query": "",

        # Feedback
        "feedback_comment": "",

        # Telegram
        "tg_api_id": "",
        "tg_api_hash": "",
        "tg_channels": "pbikni",
        "tg_phone": "",
        "tg_code": "",
        "tg_2fa": "",

        # VNS (UI only)
        "vns_login": "",
        "vns_password": "",

        # Eval state
        "last_eval_metrics": None,
        "last_eval_df": None,
        "last_eval_plots_dir": None,
    }

    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v


_init_state()


# ==========================================================
# Helpers
# ==========================================================
def _safe_int(x: str) -> Optional[int]:
    try:
        return int(x)
    except Exception:
        return None


def _parse_tg_channels(raw: str) -> List[str]:
    chans: List[str] = []
    for line in (raw or "").splitlines():
        line = line.strip()
        if line:
            chans.append(line)
    return chans


def _normalize_channel(channel: str) -> str:
    ch = (channel or "").strip()
    ch = ch.replace("https://t.me/", "").replace("http://t.me/", "").replace("t.me/", "")
    ch = ch.strip("@").strip("/").strip()
    return ch


def _maybe_load_index() -> None:
    try:
        load_faiss_index(CONFIG.faiss_index_path, CONFIG.faiss_meta_path)
        st.session_state.index_ready = True
    except Exception:
        st.session_state.index_ready = False


def _online_badge() -> None:
    if st.session_state.online_mode:
        st.sidebar.success("üü¢ ONLINE ‚Äî –¥–æ—Å—Ç—É–ø –¥–æ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É –¥–æ–∑–≤–æ–ª–µ–Ω–æ")
    else:
        st.sidebar.error("üî¥ OFFLINE ‚Äî —Ç—ñ–ª—å–∫–∏ –ª–æ–∫–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ")


def _build_llm_settings() -> LLMSettings:
    return LLMSettings(
        enabled=bool(st.session_state.llm_enabled and st.session_state.online_mode),
        provider=st.session_state.llm_provider,
        model=(st.session_state.llm_model or "").strip(),
        api_key=(st.session_state.llm_api_key or "").strip(),
        base_url=(st.session_state.llm_base_url or "").strip() or None,
        temperature=float(st.session_state.llm_temperature),
        max_tokens=650,
    )


def _provider_default_model(provider: str) -> str:
    presets = MODEL_PRESETS.get(provider) or []
    return presets[0] if presets else ""


def _ensure_valid_model_for_provider(provider: str) -> None:
    presets = MODEL_PRESETS.get(provider) or []
    if provider == "custom":
        return
    if presets and st.session_state.llm_model not in presets:
        st.session_state.llm_model = presets[0]


def _delete_file_silent(p: Path) -> bool:
    try:
        if p.exists():
            p.unlink()
        return True
    except Exception:
        return False


def _wipe_local_storage() -> dict:
    ok = True
    ok &= _delete_file_silent(Path(CONFIG.local_cache_path))
    ok &= _delete_file_silent(Path(CONFIG.faiss_index_path))
    ok &= _delete_file_silent(Path(CONFIG.faiss_meta_path))
    return {"ok": ok}


def export_report_zip() -> Optional[Path]:
    report_dir = Path("report")
    if not report_dir.exists():
        return None

    zip_path = report_dir / "report_package.zip"
    try:
        with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as z:
            for p in report_dir.rglob("*"):
                if p.is_dir():
                    continue
                if p.name.endswith(".zip"):
                    continue
                z.write(p, arcname=str(p.relative_to(report_dir)))
        return zip_path
    except Exception:
        return None


# -------- Telegram async runner ----------
def _run_async(coro, timeout_sec: int = 25):
    import asyncio
    try:
        return asyncio.run(asyncio.wait_for(coro, timeout=timeout_sec))
    except RuntimeError:
        loop = asyncio.new_event_loop()
        try:
            asyncio.set_event_loop(loop)
            return loop.run_until_complete(asyncio.wait_for(coro, timeout=timeout_sec))
        finally:
            loop.close()


# ==========================================================
# Evaluation helpers (FIXED metrics + no undefined vars)
# ==========================================================
def _load_eval_set(path: Path) -> List[dict]:
    items = safe_read_jsonl(path)
    return items


def _is_hit(chunk, rule: dict) -> bool:
    url = (chunk.url or "").lower()
    title = (chunk.title or "").lower()
    stype = (chunk.source_type or "").lower()
    text = (chunk.text or "").lower()

    must_url = (rule.get("must_contain_url") or "").lower()
    must_type = (rule.get("must_contain_type") or "").lower()
    must_text = (rule.get("must_contain_text") or "").lower()

    if must_url and must_url not in url:
        return False
    if must_type and must_type != stype:
        return False
    if must_text and must_text not in text and must_text not in title:
        return False
    return True


def _save_eval_plots(df: pd.DataFrame, out_dir: Path) -> None:
    out_dir.mkdir(parents=True, exist_ok=True)

    hit_counts = df["hit"].value_counts()
    plt.figure()
    hit_counts.plot(kind="pie", autopct="%1.1f%%")
    plt.title("Evaluation: Hit ratio")
    plt.ylabel("")
    plt.tight_layout()
    plt.savefig(out_dir / "hit_ratio.png", dpi=200)
    plt.close()

    plt.figure()
    df_hits = df[df["hit"] == True]
    if not df_hits.empty:
        df_hits["hit_rank"].value_counts().sort_index().plot(kind="bar")
        plt.title("Hit rank distribution")
        plt.xlabel("Rank of first relevant source")
        plt.ylabel("Count")
        plt.tight_layout()
        plt.savefig(out_dir / "hit_rank_hist.png", dpi=200)
    plt.close()

    plt.figure()
    df["top1_score"].dropna().plot(kind="hist", bins=10)
    plt.title("Top-1 similarity score distribution")
    plt.xlabel("Score")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.savefig(out_dir / "top1_score_hist.png", dpi=200)
    plt.close()


def run_retrieval_eval(top_k: int = 5, use_reranker: bool = False) -> dict:
    from core.index import search_index

    eval_path = Path("eval_set.jsonl")
    eval_set = _load_eval_set(eval_path)
    if not eval_set:
        return {"ok": False, "error": "eval_set.jsonl not found or empty."}

    index, meta = load_faiss_index(CONFIG.faiss_index_path, CONFIG.faiss_meta_path)

    hits = 0
    hits_at_1 = 0
    hits_at_3 = 0
    hits_at_5 = 0
    rr_sum = 0.0
    rows = []

    internal_k = max(getattr(CONFIG, "internal_k_min", 30), top_k * getattr(CONFIG, "internal_k_multiplier", 8))
    rerank_top_n = int(st.session_state.reranker_top_n_ui)

    for ex in eval_set:
        query = ex.get("query", "")

        candidates = search_index(
            query=query,
            index=index,
            chunks=meta,
            embed_model_name=CONFIG.embed_model_name,
            top_k=top_k,
            internal_k=internal_k,
            min_score=float(st.session_state.min_score),
            keyword_filter=bool(st.session_state.keyword_filter),
        )

        if use_reranker and RERANK_AVAILABLE:
            results = rerank_results(
                query=query,
                results=candidates[:rerank_top_n],
                model_name=st.session_state.reranker_model_ui,
                top_k=top_k,
            )
            mode = "reranker"
        else:
            results = candidates[:top_k]
            mode = "faiss"

        hit_rank: Optional[int] = None
        rel_count = 0

        for i, (chunk, score) in enumerate(results, start=1):
            if _is_hit(chunk, ex):
                rel_count += 1
                if hit_rank is None:
                    hit_rank = i

        hit = hit_rank is not None
        if hit:
            hits += 1
            rr_sum += 1.0 / float(hit_rank)

            if hit_rank <= 1:
                hits_at_1 += 1
            if hit_rank <= 3:
                hits_at_3 += 1
            if hit_rank <= 5:
                hits_at_5 += 1

        precision = rel_count / float(top_k) if top_k else 0.0

        rows.append({
            "query": query,
            "hit": hit,
            "hit_rank": hit_rank,
            "precision@k": round(precision, 4),
            "top1_score": float(results[0][1]) if results else None,
            "top1_url": results[0][0].url if results else None,
            "top1_type": results[0][0].source_type if results else None,
            "mode": mode,
        })

    n = len(eval_set)
    recall = hits / n if n else 0.0
    mrr = rr_sum / n if n else 0.0
    avg_prec = sum(r["precision@k"] for r in rows) / n if n else 0.0

    df = pd.DataFrame(rows)

    # score stats
    top1_scores = df["top1_score"].dropna()
    mean_top1 = float(top1_scores.mean()) if not top1_scores.empty else None
    median_top1 = float(top1_scores.median()) if not top1_scores.empty else None

    metrics = {
        "n": n,
        "top_k": top_k,
        "mode": mode,
        "recall_at_k": recall,
        "mrr_at_k": mrr,
        "avg_precision_at_k": avg_prec,
        "hit_at_1": hits_at_1 / n if n else 0.0,
        "hit_at_3": hits_at_3 / n if n else 0.0,
        "hit_at_5": hits_at_5 / n if n else 0.0,
        "top1_score_mean": mean_top1,
        "top1_score_median": median_top1,
        "min_score": float(st.session_state.min_score),
        "keyword_filter": bool(st.session_state.keyword_filter),
        "reranker_model": st.session_state.reranker_model_ui if (use_reranker and RERANK_AVAILABLE) else None,
        "reranker_top_n": int(st.session_state.reranker_top_n_ui) if (use_reranker and RERANK_AVAILABLE) else None,
        "embed_model": CONFIG.embed_model_name,
    }

    report_dir = Path("report")
    plots_dir = report_dir / "plots"
    report_dir.mkdir(exist_ok=True)
    plots_dir.mkdir(parents=True, exist_ok=True)

    try:
        df.to_csv(report_dir / "eval_results.csv", index=False, encoding="utf-8")
        (report_dir / "metrics.json").write_text(json.dumps(metrics, ensure_ascii=False, indent=2), encoding="utf-8")
        _save_eval_plots(df, plots_dir)
    except Exception:
        pass

    return {"ok": True, "metrics": metrics, "df": df, "plots_dir": str(plots_dir)}


def dataset_stats_from_cache(cache_path: Path) -> dict:
    if not cache_path.exists():
        return {"ok": False, "error": "local_cache.jsonl not found"}

    types: Dict[str, int] = {}
    dates: Dict[str, int] = {}

    with cache_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = (line or "").strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except Exception:
                continue

            stype = obj.get("source_type", "other")
            if stype:
                types[stype] = types.get(stype, 0) + 1

            d = obj.get("date")
            if d:
                dates[d] = dates.get(d, 0) + 1

    return {"ok": True, "types": types, "dates": dates}


# ==========================================================
# Sidebar UI
# ==========================================================
st.sidebar.title("‚öôÔ∏è –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è")

st.session_state.online_mode = st.sidebar.toggle(
    "Online mode (–¥–æ–∑–≤–æ–ª–∏—Ç–∏ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç-–∑–∞–ø–∏—Ç–∏)",
    value=bool(st.session_state.online_mode),
    help="Online = sync —Ç–∞ LLM. Offline = —Ç—ñ–ª—å–∫–∏ –ª–æ–∫–∞–ª—å–Ω–∞ –±–∞–∑–∞.",
)
_online_badge()


# -----------------------------
# Upload ingest
# -----------------------------
with st.sidebar.expander("üìÑ Upload PDF/DOCX (Local ingest)", expanded=False):
    uploaded_files = st.file_uploader(
        "üìé –ó–∞–≤–∞–Ω—Ç–∞–∂ PDF/DOCX —Ñ–∞–π–ª–∏",
        type=["pdf", "docx"],
        accept_multiple_files=True,
    )

    if uploaded_files and st.button("üì• Ingest uploaded files", use_container_width=True):
        try:
            import importlib
            mod = importlib.import_module("core.upload_ingest")
            ingest_uploaded_files = getattr(mod, "ingest_uploaded_files")
        except Exception as e:
            st.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ core/upload_ingest.py")
            st.exception(e)
        else:
            with st.spinner("–Ü–º–ø–æ—Ä—Ç—É—é —Ñ–∞–π–ª–∏..."):
                rep = ingest_uploaded_files(uploaded_files)

            st.success("‚úÖ –§–∞–π–ª–∏ –¥–æ–¥–∞–Ω–æ —É –±–∞–∑—É —Ç–∞ —ñ–Ω–¥–µ–∫—Å –æ–Ω–æ–≤–ª–µ–Ω–æ.")
            st.json(rep)
            _maybe_load_index()



# -----------------------------
# Document scope for local uploads
# -----------------------------
with st.sidebar.expander("üìå Document scope (local uploads)", expanded=False):
    doc_options: List[Tuple[str, str]] = []

    try:
        meta_path = Path(CONFIG.faiss_meta_path)
        if meta_path.exists():
            meta_chunks = load_chunks_from_jsonl(meta_path)

            seen = set()
            for ch in meta_chunks:
                if (ch.source_type or "").lower() != "local":
                    continue

                extra = ch.extra or {}
                doc_id = extra.get("doc_id")
                if not doc_id:
                    continue
                if doc_id in seen:
                    continue
                seen.add(doc_id)

                label = extra.get("file_name") or extra.get("saved_as") or doc_id
                doc_options.append((label, doc_id))
    except Exception:
        doc_options = []

    st.session_state.doc_scope_enabled = st.checkbox(
        "–í—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –ø–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–º —Ñ–∞–π–ª–∞–º",
        value=bool(st.session_state.doc_scope_enabled),
    )

    chosen = st.multiselect(
        "–û–±—Ä–∞—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ —Ñ–∞–π–ª–∏",
        options=[x[1] for x in doc_options],
        format_func=lambda did: next((lbl for lbl, _id in doc_options if _id == did), did),
        disabled=not st.session_state.doc_scope_enabled,
    )
    st.session_state.doc_scope_ids = set(chosen)


# -----------------------------
# Retrieval tuning
# -----------------------------
with st.sidebar.expander("üß≤ Retrieval tuning (FAISS)", expanded=False):
    st.session_state.min_score = st.slider(
        "min_score",
        0.0,
        1.0,
        float(st.session_state.min_score),
        0.01,
    )
    st.session_state.keyword_filter = st.checkbox("keyword_filter", value=bool(st.session_state.keyword_filter))
    st.session_state.show_retrieval_debug = st.checkbox(
        "–ü–æ–∫–∞–∑–∞—Ç–∏ retrieval debug",
        value=bool(st.session_state.show_retrieval_debug),
    )


# -----------------------------
# Source filters
# -----------------------------
with st.sidebar.expander("üß© Filters (source types)", expanded=False):
    st.caption("–§—ñ–ª—å—Ç—Ä—É—î –¥–∂–µ—Ä–µ–ª–∞ –ø–µ—Ä–µ–¥ rerank/answer.")
    st.session_state.filter_lpnu = st.checkbox("LPNU", value=bool(st.session_state.filter_lpnu))
    st.session_state.filter_tg = st.checkbox("Telegram", value=bool(st.session_state.filter_tg))
    st.session_state.filter_vns = st.checkbox("VNS", value=bool(st.session_state.filter_vns))
    st.session_state.filter_local = st.checkbox("Local", value=bool(st.session_state.filter_local))


# -----------------------------
# Answer UI
# -----------------------------
with st.sidebar.expander("üßæ Answer UI", expanded=False):
    st.session_state.show_used_sources_only = st.checkbox(
        "–ü–æ–∫–∞–∑—É–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –¥–∂–µ—Ä–µ–ª–∞, —è–∫—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–ª–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å",
        value=bool(st.session_state.show_used_sources_only),
        help="–ü—Ä–∞—Ü—é—î –Ω–∞–π–∫—Ä–∞—â–µ –∑ LLM, –±–æ —î —Ü–∏—Ç–∞—Ç–∏ [1],[2]...",
    )
    st.session_state.show_chunk_preview = st.checkbox(
        "–ü–æ–∫–∞–∑—É–≤–∞—Ç–∏ –ø—Ä–µ–≤‚Äô—é chunk‚Äô—ñ–≤",
        value=bool(st.session_state.show_chunk_preview),
    )


# -----------------------------
# Reranker
# -----------------------------
with st.sidebar.expander("üéØ Reranker (–ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è Top-K)", expanded=False):
    if not RERANK_AVAILABLE:
        st.warning("Reranker –º–æ–¥—É–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ (`core/rerank.py`). –§—É–Ω–∫—Ü—ñ—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

    st.session_state.use_reranker_ui = st.checkbox(
        "–£–≤—ñ–º–∫–Ω—É—Ç–∏ Reranker",
        value=bool(st.session_state.use_reranker_ui),
        disabled=not RERANK_AVAILABLE,
        help="–ü–µ—Ä–µ—Ä–∞–Ω–∂—É—î —Ç–æ–ø-N –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –¥–ª—è –∫—Ä–∞—â–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ.",
    )

    st.session_state.reranker_model_ui = st.text_input(
        "Reranker model",
        value=st.session_state.reranker_model_ui,
        disabled=not (RERANK_AVAILABLE and st.session_state.use_reranker_ui),
        help="–ù–∞–ø—Ä.: cross-encoder/ms-marco-MiniLM-L-6-v2",
    )

    st.session_state.reranker_top_n_ui = st.slider(
        "Reranker candidates (top-N)",
        min_value=10,
        max_value=100,
        value=int(st.session_state.reranker_top_n_ui),
        step=5,
        disabled=not (RERANK_AVAILABLE and st.session_state.use_reranker_ui),
    )

    st.caption("‚ÑπÔ∏è Reranker –ø—Ä–∞—Ü—é—î –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ, –∞–ª–µ –¥–∞—î –ø–æ–º—ñ—Ç–Ω–æ –∫—Ä–∞—â—ñ —Ç–æ–ø-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏.")


# -----------------------------
# VNS (UI only)
# -----------------------------
with st.sidebar.expander("üîê –í–ù–° (–æ–ø—Ü—ñ–π–Ω–æ, –±–µ–∑ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è)", expanded=False):
    st.caption(
        "–õ–æ–≥—ñ–Ω —ñ –ø–∞—Ä–æ–ª—å –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—ñ–π –ø–∞–º º—è—Ç—ñ (session_state). "
        "–ù–µ –∑–∞–ø–∏—Å—É—é—Ç—å—Å—è —É —Ñ–∞–π–ª–∏."
    )
    st.session_state.vns_login = st.text_input("VNS login", value=st.session_state.vns_login)
    st.session_state.vns_password = st.text_input("VNS password", value=st.session_state.vns_password, type="password")

    st.write("**–ó–∞—Ä–∞–∑ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ —Å–µ—Å—ñ—ó:**")
    st.write(f"Login: `{st.session_state.vns_login}`")
    st.write(f"Password: `{mask_secret(st.session_state.vns_password)}`")

    if st.button("üßπ –û—á–∏—Å—Ç–∏—Ç–∏ VNS –∫—Ä–µ–¥–µ–Ω—à–∞–ª–∏", use_container_width=True):
        st.session_state.vns_login = ""
        st.session_state.vns_password = ""
        st.success("–ö—Ä–µ–¥–µ–Ω—à–∞–ª–∏ –æ—á–∏—â–µ–Ω–æ.")


# -----------------------------
# Telegram (Auth + Test)
# -----------------------------
with st.sidebar.expander("üì° Telegram —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è (Auth + Sync)", expanded=False):
    st.caption(
        "Telethon –ø–æ—Ç—Ä–µ–±—É—î **–æ–¥–∏–Ω —Ä–∞–∑** –∞–≤—Ç–æ—Ä–∏–∑—É–≤–∞—Ç–∏ —Å–µ—Å—ñ—é. "
        "–ü—ñ—Å–ª—è —Ü—å–æ–≥–æ ingest/sync –ø—Ä–∞—Ü—é—î –±–µ–∑ —Ç–µ–ª–µ—Ñ–æ–Ω—É —Ç–∞ –∫–æ–¥—É.\n\n"
        "–°–µ—Å—ñ—è –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –ª–æ–∫–∞–ª—å–Ω–æ —É —Ñ–∞–π–ª—ñ: `data/tg_session.session` (–Ω–µ –∫–æ–º—ñ—Ç–∏—Ç–∏ –≤ Git)."
    )

    st.session_state.tg_api_id = st.text_input("Telegram API ID", value=st.session_state.tg_api_id)
    st.session_state.tg_api_hash = st.text_input("Telegram API HASH", value=st.session_state.tg_api_hash, type="password")
    st.session_state.tg_channels = st.text_area("Telegram channels (one per line)", value=st.session_state.tg_channels)

    api_id_int = _safe_int(st.session_state.tg_api_id.strip()) if st.session_state.tg_api_id.strip() else None
    api_hash_str = st.session_state.tg_api_hash.strip() if st.session_state.tg_api_hash.strip() else None

    st.divider()
    st.subheader("üîê Telegram –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è (1 —Ä–∞–∑)")

    st.session_state.tg_phone = st.text_input("–¢–µ–ª–µ—Ñ–æ–Ω (+380...)", value=st.session_state.tg_phone)
    st.session_state.tg_code = st.text_input("–ö–æ–¥ –∑ Telegram", value=st.session_state.tg_code)
    st.session_state.tg_2fa = st.text_input("2FA –ø–∞—Ä–æ–ª—å (—è–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–æ)", value=st.session_state.tg_2fa, type="password")

    colA, colB = st.columns(2)
    send_code_btn = colA.button("üì® –ù–∞–¥—ñ—Å–ª–∞—Ç–∏ –∫–æ–¥", disabled=not st.session_state.online_mode)
    sign_in_btn = colB.button("‚úÖ –ü—ñ–¥—Ç–≤–µ—Ä–¥–∏—Ç–∏ –∫–æ–¥", disabled=not st.session_state.online_mode)

    if send_code_btn:
        if not api_id_int or not api_hash_str or not st.session_state.tg_phone.strip():
            st.error("–í–∫–∞–∂–∏ api_id, api_hash —ñ –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω—É.")
        else:
            try:
                from telethon import TelegramClient

                async def _send_code():
                    async with TelegramClient("data/tg_session", api_id_int, api_hash_str) as client:
                        await client.send_code_request(st.session_state.tg_phone.strip())
                        return True

                with st.spinner("–ù–∞–¥—Å–∏–ª–∞—é –∫–æ–¥..."):
                    _run_async(_send_code(), timeout_sec=25)
                st.success("‚úÖ –ö–æ–¥ –Ω–∞–¥—ñ—Å–ª–∞–Ω–æ. –í–≤–µ–¥–∏ –∫–æ–¥ —Ç–∞ –Ω–∞—Ç–∏—Å–Ω–∏ '–ü—ñ–¥—Ç–≤–µ—Ä–¥–∏—Ç–∏ –∫–æ–¥'.")
            except Exception as e:
                st.error(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –Ω–∞–¥—ñ—Å–ª–∞—Ç–∏ –∫–æ–¥: {e}")

    if sign_in_btn:
        if not api_id_int or not api_hash_str or not st.session_state.tg_phone.strip() or not st.session_state.tg_code.strip():
            st.error("–í–∫–∞–∂–∏ api_id, api_hash, —Ç–µ–ª–µ—Ñ–æ–Ω —ñ –∫–æ–¥.")
        else:
            try:
                from telethon import TelegramClient
                from telethon.errors import SessionPasswordNeededError, PhoneCodeInvalidError

                async def _sign_in():
                    async with TelegramClient("data/tg_session", api_id_int, api_hash_str) as client:
                        try:
                            await client.sign_in(phone=st.session_state.tg_phone.strip(), code=st.session_state.tg_code.strip())
                            return {"ok": True, "msg": "‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞! –°–µ—Å—ñ—è –∑–±–µ—Ä–µ–∂–µ–Ω–∞."}
                        except SessionPasswordNeededError:
                            if not st.session_state.tg_2fa.strip():
                                return {"ok": False, "msg": "‚ö†Ô∏è –£–≤—ñ–º–∫–Ω–µ–Ω–æ 2FA. –í–≤–µ–¥–∏ –ø–∞—Ä–æ–ª—å —ñ –ø–æ–≤—Ç–æ—Ä–∏."}
                            await client.sign_in(password=st.session_state.tg_2fa.strip())
                            return {"ok": True, "msg": "‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞ (2FA). –°–µ—Å—ñ—è –∑–±–µ—Ä–µ–∂–µ–Ω–∞."}
                        except PhoneCodeInvalidError:
                            return {"ok": False, "msg": "‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –∫–æ–¥."}

                with st.spinner("–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è..."):
                    out = _run_async(_sign_in(), timeout_sec=35)
                if out["ok"]:
                    st.success(out["msg"])
                else:
                    st.warning(out["msg"])
            except Exception as e:
                st.error(f"‚ùå –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è –Ω–µ –≤–¥–∞–ª–∞—Å—è: {e}")

    st.divider()
    st.subheader("üß™ Test Telegram (last 3 msgs)")

    test_btn = st.button("üß™ Test Telegram (show last 3 msgs)", disabled=not st.session_state.online_mode)

    if test_btn:
        channels = _parse_tg_channels(st.session_state.tg_channels)
        if not api_id_int or not api_hash_str or not channels:
            st.error("–í–≤–µ–¥–∏ api_id, api_hash —ñ —Ö–æ—á–∞ –± 1 –∫–∞–Ω–∞–ª (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ pbikni).")
        else:
            try:
                from telethon import TelegramClient

                async def _test_channel():
                    ch = _normalize_channel(channels[0])
                    async with TelegramClient("data/tg_session", api_id_int, api_hash_str) as client:
                        is_auth = await client.is_user_authorized()
                        if not is_auth:
                            return {"ok": False, "err": "‚ùå –°–µ—Å—ñ—è –ù–ï –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–∞. –°–ø–æ—á–∞—Ç–∫—É –∑—Ä–æ–±–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—é –≤–∏—â–µ."}

                        entity = await client.get_entity(ch)
                        msgs = []
                        async for m in client.iter_messages(entity, limit=3):
                            txt = (getattr(m, "message", None) or "").strip()
                            if not txt:
                                continue
                            msgs.append((m.id, m.date, txt))
                        return {"ok": True, "channel": ch, "msgs": msgs}

                with st.spinner("–¢–µ—Å—Ç—É—é Telegram (–¥–æ 25 —Å–µ–∫)..."):
                    out = _run_async(_test_channel(), timeout_sec=25)

                if not out["ok"]:
                    st.error(out["err"])
                else:
                    st.success(f"‚úÖ –ö–∞–Ω–∞–ª –¥–æ—Å—Ç—É–ø–Ω–∏–π: {out['channel']}")
                    if not out["msgs"]:
                        st.info("–ù–µ–º–∞—î —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å —É –æ—Å—Ç–∞–Ω–Ω—ñ—Ö 3 –∞–±–æ –≤–æ–Ω–∏ –ø–æ—Ä–æ–∂–Ω—ñ.")
                    for mid, dt, txt in out["msgs"]:
                        st.write(f"**{mid}** ‚Ä¢ {dt}  \n{txt}")

            except Exception as e:
                st.error(f"Telegram test failed: {e}")


# -----------------------------
# LLM settings
# -----------------------------
with st.sidebar.expander("ü§ñ LLM —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è (–æ–ø—Ü—ñ–π–Ω–æ)", expanded=False):
    st.caption(
        "RAG + LLM –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è. –ü—Ä–∞—Ü—é—î —á–µ—Ä–µ–∑ OpenAI-compatible API: OpenAI / Groq / OpenRouter / Ollama / Custom.\n"
        "üîí –ö–ª—é—á –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –ª–∏—à–µ –≤ session_state."
    )

    st.session_state.llm_enabled = st.checkbox(
        "–£–≤—ñ–º–∫–Ω—É—Ç–∏ LLM",
        value=bool(st.session_state.llm_enabled),
        disabled=not st.session_state.online_mode,
        help="–ü–æ—Ç—Ä—ñ–±–µ–Ω Online mode.",
    )

    provider = st.selectbox(
        "–ü—Ä–æ–≤–∞–π–¥–µ—Ä",
        options=PROVIDERS,
        index=PROVIDERS.index(st.session_state.llm_provider),
        disabled=not st.session_state.online_mode,
    )

    if provider != st.session_state.llm_provider:
        st.session_state.llm_provider = provider
        st.session_state.llm_model = _provider_default_model(provider)

    _ensure_valid_model_for_provider(st.session_state.llm_provider)

    preset_models = MODEL_PRESETS.get(st.session_state.llm_provider, [])
    st.session_state.use_custom_model = st.checkbox(
        "–í–∫–∞–∑–∞—Ç–∏ –º–æ–¥–µ–ª—å –≤—Ä—É—á–Ω—É",
        value=bool(st.session_state.use_custom_model),
        disabled=not st.session_state.online_mode,
    )

    if (not st.session_state.use_custom_model) and preset_models:
        options = preset_models[:]
        if st.session_state.llm_model and st.session_state.llm_model not in options:
            options = [st.session_state.llm_model] + options

        selected_model = st.selectbox(
            "–ú–æ–¥–µ–ª—å (–≤–∏–±–µ—Ä–∏ –∑—ñ —Å–ø–∏—Å–∫—É)",
            options=options,
            index=options.index(st.session_state.llm_model) if st.session_state.llm_model in options else 0,
            disabled=not st.session_state.online_mode,
        )
        st.session_state.llm_model = selected_model
    else:
        st.session_state.llm_model = st.text_input(
            "–ú–æ–¥–µ–ª—å (–≤—Ä—É—á–Ω—É)",
            value=st.session_state.llm_model,
            disabled=not st.session_state.online_mode,
        )

    st.session_state.llm_api_key = st.text_input(
        "API Key",
        value=st.session_state.llm_api_key,
        type="password",
        disabled=(not st.session_state.online_mode) or (st.session_state.llm_provider == "ollama"),
    )

    st.session_state.llm_base_url = st.text_input(
        "Custom Base URL (—Ç—ñ–ª—å–∫–∏ –¥–ª—è custom)",
        value=st.session_state.llm_base_url,
        disabled=(not st.session_state.online_mode) or (st.session_state.llm_provider != "custom"),
        help="–ù–∞–ø—Ä.: https://your-openai-compatible-endpoint/v1",
    )

    st.session_state.llm_temperature = st.slider(
        "Temperature",
        min_value=0.0,
        max_value=1.0,
        value=float(st.session_state.llm_temperature),
        step=0.05,
        disabled=not st.session_state.online_mode,
    )

    st.session_state.llm_debug = st.checkbox(
        "–ü–æ–∫–∞–∑–∞—Ç–∏ debug (URL + –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±–µ–∑ –∫–ª—é—á–∞)",
        value=bool(st.session_state.llm_debug),
        disabled=not st.session_state.online_mode,
    )

    if st.session_state.llm_debug:
        base_url = build_base_url(st.session_state.llm_provider, st.session_state.llm_base_url or None)
        st.code(f"Request URL: {base_url}/chat/completions", language="text")

    if st.button("üß™ Test LLM", disabled=not (st.session_state.online_mode and st.session_state.llm_enabled)):
        llm = _build_llm_settings()
        try:
            test_out = chat_completion(
                llm,
                messages=[
                    {"role": "system", "content": "–¢–∏ —Ç–µ—Å—Ç–æ–≤–∏–π –ø–æ–º—ñ—á–Ω–∏–∫."},
                    {"role": "user", "content": "–ù–∞–ø–∏—à–∏ 'OK' —ñ –ø–æ—Ç–æ—á–Ω—É –¥–∞—Ç—É —É —Ñ–æ—Ä–º–∞—Ç—ñ YYYY-MM-DD."},
                ],
            )
            st.success("‚úÖ LLM –ø—Ä–∞—Ü—é—î!")
            st.write(test_out)
        except Exception as e:
            st.error(f"LLM test failed: {e}")


# ==========================================================
# Index actions
# ==========================================================
st.sidebar.divider()

if st.sidebar.button("üì¶ –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏/–∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–∏–π —ñ–Ω–¥–µ–∫—Å"):
    try:
        load_faiss_index(CONFIG.faiss_index_path, CONFIG.faiss_meta_path)
        st.session_state.index_ready = True
        st.sidebar.success("FAISS —ñ–Ω–¥–µ–∫—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ ‚úÖ")
    except Exception as e:
        st.session_state.index_ready = False
        st.sidebar.error(f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω–¥–µ–∫—Å: {e}")

if st.sidebar.button("üõ†Ô∏è –ü–æ–±—É–¥—É–≤–∞—Ç–∏ —ñ–Ω–¥–µ–∫—Å –∑ local_cache.jsonl"):
    chunks = load_chunks_from_jsonl(CONFIG.local_cache_path)
    if not chunks:
        st.sidebar.error(
            "local_cache.jsonl –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –Ω–µ —ñ—Å–Ω—É—î. "
            "–ù–∞—Ç–∏—Å–Ω–∏ 'Sync knowledge base' –∞–±–æ –¥–æ–¥–∞–π –¥–æ–∫—É–º–µ–Ω—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ."
        )
    else:
        try:
            build_faiss_index(
                chunks=chunks,
                embed_model_name=CONFIG.embed_model_name,
                index_path=CONFIG.faiss_index_path,
                meta_path=CONFIG.faiss_meta_path,
            )
            st.session_state.index_ready = True
            st.sidebar.success("–Ü–Ω–¥–µ–∫—Å —É—Å–ø—ñ—à–Ω–æ –ø–æ–±—É–¥–æ–≤–∞–Ω–æ ‚úÖ")
        except Exception as e:
            st.sidebar.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–æ–±—É–¥–æ–≤–∏ —ñ–Ω–¥–µ–∫—Å—É: {e}")


# ==========================================================
# Sync knowledge base
# ==========================================================
if st.sidebar.button("üîÑ Sync knowledge base (LPNU + TG + rebuild index)", disabled=not st.session_state.online_mode):
    from pipelines.sync_all import sync_all

    channels = _parse_tg_channels(st.session_state.tg_channels)
    api_id = _safe_int(st.session_state.tg_api_id.strip()) if st.session_state.tg_api_id.strip() else None
    api_hash = st.session_state.tg_api_hash.strip() if st.session_state.tg_api_hash.strip() else None

    with st.spinner("–°–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—è –∑–Ω–∞–Ω—å..."):
        report = sync_all(
            api_id=api_id,
            api_hash=api_hash,
            channels=channels if (api_id and api_hash and channels) else None,
        )

    st.session_state.last_sync_report = report
    st.sidebar.success("Sync –∑–∞–≤–µ—Ä—à–µ–Ω–æ ‚úÖ")
    st.sidebar.json(report)
    _maybe_load_index()


# ==========================================================
# Advanced wipe
# ==========================================================
st.sidebar.divider()
with st.sidebar.expander("üß® Advanced: wipe local storage", expanded=False):
    st.caption("–í–∏–¥–∞–ª—è—î local_cache.jsonl + FAISS index. –ö–æ—Ä–∏—Å–Ω–æ –¥–ª—è —á–∏—Å—Ç–∏—Ö —Ç–µ—Å—Ç—ñ–≤.")
    confirm = st.checkbox("–Ø —Ä–æ–∑—É–º—ñ—é —â–æ —Ü–µ –≤–∏–¥–∞–ª–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ")
    if st.button("üóëÔ∏è Wipe local cache + index", disabled=not confirm):
        r = _wipe_local_storage()
        st.session_state.index_ready = False
        st.session_state.last_results = []
        st.session_state.last_sync_report = None
        if r.get("ok"):
            st.success("‚úÖ –í–∏–¥–∞–ª–µ–Ω–æ. –¢–µ–ø–µ—Ä –º–æ–∂–Ω–∞ –∑—Ä–æ–±–∏—Ç–∏ Sync –∑ –Ω—É–ª—è.")
        else:
            st.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–¥–∞–ª–∏—Ç–∏ –≤—Å—ñ —Ñ–∞–π–ª–∏ (–º–æ–∂—É—Ç—å –±—É—Ç–∏ –≤—ñ–¥–∫—Ä–∏—Ç—ñ).")


# ==========================================================
# UI reset
# ==========================================================
st.sidebar.divider()
if st.sidebar.button("üßπ –û—á–∏—Å—Ç–∏—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ—à—É–∫—É (UI)"):
    st.session_state.last_results = []
    st.session_state.last_answer_struct = None
    st.success("–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ—á–∏—â–µ–Ω–æ.")

if st.sidebar.button("üß® –ü–æ–≤–Ω–∏–π —Å–∫–∏–¥–∞–Ω–Ω—è (–æ—á–∏—Å—Ç–∏—Ç–∏ UI + –∫—Ä–µ–¥–µ–Ω—à–∞–ª–∏)"):
    st.session_state.last_results = []
    st.session_state.last_answer_struct = None
    st.session_state.vns_login = ""
    st.session_state.vns_password = ""
    st.session_state.tg_api_id = ""
    st.session_state.tg_api_hash = ""
    st.session_state.tg_phone = ""
    st.session_state.tg_code = ""
    st.session_state.tg_2fa = ""
    st.session_state.llm_api_key = ""
    st.session_state.last_sync_report = None
    st.session_state.last_eval_metrics = None
    st.session_state.last_eval_df = None
    st.session_state.last_eval_plots_dir = None
    st.success("–°–µ—Å—ñ—è –æ—á–∏—â–µ–Ω–∞ (–±–µ–∑ –≤–∏–¥–∞–ª–µ–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤).")


# ==========================================================
# Main UI
# ==========================================================
st.title("üéì IKNI Assistant ‚Äî RAG MVP (Streamlit)")

st.caption(
    "MVP: –ª–æ–∫–∞–ª—å–Ω–∏–π —ñ–Ω–¥–µ–∫—Å + retrieval + –≤—ñ–¥–ø–æ–≤—ñ–¥—å (offline –∞–±–æ —á–µ—Ä–µ–∑ LLM). "
    "–Ñ –∞–≤—Ç–æ-–∞—Ä—Ö—ñ–≤ –¥–∞–Ω–∏—Ö (LPNU + Wiki + Telegram) —á–µ—Ä–µ–∑ 'Sync knowledge base'."
)

if st.session_state.online_mode:
    st.info("üü¢ ONLINE —Ä–µ–∂–∏–º —É–≤—ñ–º–∫–Ω–µ–Ω–æ ‚Äî –º–æ–∂–Ω–∞ —Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ LLM.")
else:
    st.warning("üî¥ OFFLINE —Ä–µ–∂–∏–º ‚Äî –ø—Ä–∞—Ü—é—î —Ç—ñ–ª—å–∫–∏ –ª–æ–∫–∞–ª—å–Ω–∞ –±–∞–∑–∞ —Ç–∞ —ñ–Ω–¥–µ–∫—Å.")

if not Path(CONFIG.faiss_index_path).exists():
    st.warning(
        "FAISS —ñ–Ω–¥–µ–∫—Å —â–µ –Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ. –ù–∞—Ç–∏—Å–Ω–∏ **Sync knowledge base** —É —Å–∞–π–¥–±–∞—Ä—ñ (Online mode), "
        "—â–æ–± –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ü–ö–ù–Ü —Ç–∞ –ø–æ–±—É–¥—É–≤–∞—Ç–∏ —ñ–Ω–¥–µ–∫—Å."
    )

if Path(CONFIG.faiss_index_path).exists() and not st.session_state.index_ready:
    _maybe_load_index()

if st.session_state.last_sync_report:
    with st.expander("üìÑ –û—Å—Ç–∞–Ω–Ω—ñ–π Sync report", expanded=False):
        st.json(st.session_state.last_sync_report)

tab_chat, tab_eval = st.tabs(["üí¨ Chat / Search", "üìä Metrics & Evaluation"])


# ==========================================================
# TAB 1: Chat / Search
# ==========================================================
with tab_chat:
    st.subheader("‚ö° –®–≤–∏–¥–∫—ñ –∑–∞–ø–∏—Ç–∏")

    qcol1, qcol2, qcol3, qcol4 = st.columns(4)
    if qcol1.button("–•—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä –Ü–ö–ù–Ü?"):
        st.session_state.quick_query = "–•—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä –Ü–ö–ù–Ü?"
    if qcol2.button("–ö–æ–ª–∏ —Å—Ç–≤–æ—Ä–µ–Ω–æ –Ü–ö–ù–Ü?"):
        st.session_state.quick_query = "–ö–æ–ª–∏ —Å—Ç–≤–æ—Ä–µ–Ω–æ –Ü–ö–ù–Ü?"
    if qcol3.button("–ö–µ—Ä—ñ–≤–Ω–∏—Ü—Ç–≤–æ –Ü–ö–ù–Ü"):
        st.session_state.quick_query = "–•—Ç–æ –≤—Ö–æ–¥–∏—Ç—å –≤ –∫–µ—Ä—ñ–≤–Ω–∏—Ü—Ç–≤–æ –Ü–ö–ù–Ü?"
    if qcol4.button("–©–æ –Ω–æ–≤–æ–≥–æ –≤ pbikni?"):
        st.session_state.quick_query = "–©–æ –Ω–æ–≤–æ–≥–æ –≤ Telegram –∫–∞–Ω–∞–ª—ñ pbikni?"

    default_query = st.session_state.get("quick_query", "")

    col1, col2 = st.columns([2, 1], gap="large")

    with col1:
        st.subheader("üîé –ó–∞–ø–∏—Ç")

        query = st.text_input("–í–≤–µ–¥–∏ –ø–∏—Ç–∞–Ω–Ω—è", value=default_query)
        top_k = st.slider("Top-K –¥–∂–µ—Ä–µ–ª", min_value=1, max_value=10, value=int(CONFIG.top_k))

        use_llm = bool(st.session_state.online_mode and st.session_state.llm_enabled)

        if use_llm:
            st.caption(
                f"ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è: **LLM ON** ‚Ä¢ provider=`{st.session_state.llm_provider}` ‚Ä¢ model=`{st.session_state.llm_model}`"
            )
        else:
            st.caption("üìå –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è: **LLM OFF** (offline summarizer)")

        if st.session_state.use_reranker_ui and RERANK_AVAILABLE:
            st.caption(
                f"üéØ Reranker: **ON** ‚Ä¢ model=`{st.session_state.reranker_model_ui}` ‚Ä¢ topN={st.session_state.reranker_top_n_ui}"
            )
        else:
            st.caption("üéØ Reranker: **OFF**")

        ask_btn = st.button("–û—Ç—Ä–∏–º–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å", type="primary", use_container_width=True)

        if ask_btn:
            st.session_state.quick_query = ""
            st.session_state.last_query = query

            if not query.strip():
                st.warning("–í–≤–µ–¥–∏ –∑–∞–ø–∏—Ç.")
            elif not st.session_state.index_ready:
                st.warning("–°–ø–µ—Ä—à—É –∑–∞–≤–∞–Ω—Ç–∞–∂ –∞–±–æ –ø–æ–±—É–¥—É–π –ª–æ–∫–∞–ª—å–Ω–∏–π FAISS —ñ–Ω–¥–µ–∫—Å —É —Å–∞–π–¥–±–∞—Ä—ñ.")
            else:
                from core.index import search_index

                index, meta = load_faiss_index(CONFIG.faiss_index_path, CONFIG.faiss_meta_path)

                internal_k = max(
                    getattr(CONFIG, "internal_k_min", 30),
                    top_k * getattr(CONFIG, "internal_k_multiplier", 8),
                )

                # raw candidates
                candidates = search_index(
                    query=query,
                    index=index,
                    chunks=meta,
                    embed_model_name=CONFIG.embed_model_name,
                    top_k=top_k,
                    internal_k=internal_k,
                    min_score=float(st.session_state.min_score),
                    keyword_filter=bool(st.session_state.keyword_filter),
                )

                # ---------------------------------------
                # Source type filter
                # ---------------------------------------
                allowed_types = set()
                if st.session_state.filter_lpnu:
                    allowed_types.add("lpnu")
                if st.session_state.filter_tg:
                    allowed_types.add("tg")
                if st.session_state.filter_vns:
                    allowed_types.add("vns")
                if st.session_state.filter_local:
                    allowed_types.add("local")

                filtered_candidates = [
                    (ch, sc) for (ch, sc) in candidates
                    if (ch.source_type or "").lower() in allowed_types
                ]
                if not filtered_candidates:
                    filtered_candidates = candidates

                # ---------------------------------------
                # Document scope filter (local uploads)
                # ---------------------------------------
                if st.session_state.doc_scope_enabled and st.session_state.doc_scope_ids:
                    scoped = []
                    allowed_doc_ids = set(st.session_state.doc_scope_ids)

                    for ch, sc in filtered_candidates:
                        extra = ch.extra or {}
                        doc_id = extra.get("doc_id")
                        if doc_id and doc_id in allowed_doc_ids:
                            scoped.append((ch, sc))

                    if scoped:
                        filtered_candidates = scoped

                # ---------------------------------------
                # Rerank (optional)
                # ---------------------------------------
                if st.session_state.use_reranker_ui and RERANK_AVAILABLE:
                    results = rerank_results(
                        query=query,
                        results=filtered_candidates[: int(st.session_state.reranker_top_n_ui)],
                        model_name=st.session_state.reranker_model_ui,
                        top_k=top_k,
                    )
                    ranking_mode = "reranker"
                else:
                    results = filtered_candidates[:top_k]
                    ranking_mode = "faiss"

                st.session_state.last_results = results

                # ---------------------------------------
                # Answer
                # ---------------------------------------
                if use_llm:
                    llm = _build_llm_settings()
                    ans = make_answer_with_llm_struct(query, results, llm)
                else:
                    ans = make_answer_no_llm_struct(query, results)

                st.session_state.last_answer_struct = ans

                st.markdown(ans.markdown, unsafe_allow_html=True)

                if getattr(ans, "warnings", None):
                    st.warning(" | ".join(ans.warnings))

                st.divider()
                st.markdown("### üëçüëé Feedback")

                comment = st.text_input("–ö–æ–º–µ–Ω—Ç–∞—Ä (–æ–ø—Ü—ñ–π–Ω–æ)", key="feedback_comment")

                c1, c2, _ = st.columns([1, 1, 3])
                good = c1.button("üëç –î–æ–±—Ä–µ", use_container_width=True)
                bad = c2.button("üëé –ü–æ–≥–∞–Ω–æ", use_container_width=True)

                if good or bad:
                    rating = +1 if good else -1
                    payload = _feedback_payload(rating=rating, comment=st.session_state.get("feedback_comment", ""))
                    _append_feedback(payload)
                    st.success("‚úÖ –î—è–∫—É—é! –§—ñ–¥–±–µ–∫ –∑–±–µ—Ä–µ–∂–µ–Ω–æ.")
                    st.session_state.feedback_comment = ""

                if st.session_state.show_retrieval_debug:
                    with st.expander("üß™ Retrieval debug", expanded=False):
                        st.write(
                            {
                                "internal_k_candidates": internal_k,
                                "candidates_after_search": len(candidates),
                                "filtered_candidates": len(filtered_candidates),
                                "top_k_returned": len(results),
                                "ranking_mode": ranking_mode,
                                "min_score": float(st.session_state.min_score),
                                "keyword_filter": bool(st.session_state.keyword_filter),
                                "allowed_types": sorted(list(allowed_types)),
                                "doc_scope_enabled": bool(st.session_state.doc_scope_enabled),
                                "doc_scope_ids": sorted(list(st.session_state.doc_scope_ids or [])),
                                "used_sources": getattr(ans, "used_sources", []),
                                "warnings": getattr(ans, "warnings", []),
                            }
                        )

    with col2:
        st.subheader("üìö –î–∂–µ—Ä–µ–ª–∞")

        if not st.session_state.last_results:
            st.info("–ü—ñ—Å–ª—è –∑–∞–ø–∏—Ç—É —Ç—É—Ç –∑ º—è–≤–ª—è—Ç—å—Å—è –∑–Ω–∞–π–¥–µ–Ω—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ (top-K).")
        else:
            ans = st.session_state.last_answer_struct
            used = set(getattr(ans, "used_sources", []) or []) if ans else set()

            rows = []
            for rank, (chunk, score) in enumerate(st.session_state.last_results, start=1):
                if st.session_state.show_used_sources_only and used and (rank not in used):
                    continue

                rows.append(
                    {
                        "Rank": rank,
                        "Used": "‚úÖ" if (rank in used) else "",
                        "Score": round(float(score), 4),
                        "Title": chunk.title,
                        "Type": chunk.source_type,
                        "Date": chunk.date,
                        "URL": chunk.url,
                        "DocID": (chunk.extra or {}).get("doc_id"),
                        "Text (snippet)": (chunk.text[:180] + "‚Ä¶") if len(chunk.text) > 180 else chunk.text,
                    }
                )

            df = pd.DataFrame(rows)
            st.dataframe(df, use_container_width=True, hide_index=True)

            if st.session_state.show_chunk_preview:
                st.markdown("#### üîç –ü–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç chunk‚Äô—ñ–≤")
                for rank, (chunk, score) in enumerate(st.session_state.last_results, start=1):
                    if st.session_state.show_used_sources_only and used and (rank not in used):
                        continue

                    label = f"[{rank}] {chunk.title} ({chunk.source_type}) score={score:.3f}"
                    with st.expander(label, expanded=False):
                        if chunk.url:
                            st.write(chunk.url)
                        if chunk.date:
                            st.write(chunk.date)

                        extra = chunk.extra or {}
                        if extra.get("doc_id"):
                            st.caption(f"doc_id: `{extra.get('doc_id')}`")

                        st.write(chunk.text)


# ==========================================================
# TAB 2: Metrics & Evaluation
# ==========================================================
with tab_eval:
    st.subheader("üìä Metrics & Evaluation")
    st.write("–û—Ü—ñ–Ω–∫–∞ retrieval (Recall/MRR/Precision) –Ω–∞ eval_set.jsonl + –≥—Ä–∞—Ñ—ñ–∫–∏ –¥–ª—è –∑–≤—ñ—Ç—É.")

    eval_k = st.slider("Evaluation K (top-K)", min_value=1, max_value=10, value=5)

    use_reranker_eval = st.checkbox(
        "–í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ Reranker –ø—ñ–¥ —á–∞—Å evaluation",
        value=bool(st.session_state.use_reranker_ui and RERANK_AVAILABLE),
        disabled=not RERANK_AVAILABLE,
    )

    if st.button("üöÄ Run evaluation", type="primary"):
        if not st.session_state.index_ready:
            st.error("–°–ø–µ—Ä—à—É –∑–∞–≤–∞–Ω—Ç–∞–∂/–ø–æ–±—É–¥—É–π —ñ–Ω–¥–µ–∫—Å.")
        elif not Path("eval_set.jsonl").exists():
            st.error("–§–∞–π–ª `eval_set.jsonl` –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        else:
            with st.spinner("–û—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è retrieval..."):
                out = run_retrieval_eval(top_k=eval_k, use_reranker=use_reranker_eval)

            if out.get("ok"):
                st.session_state.last_eval_metrics = out["metrics"]
                st.session_state.last_eval_df = out["df"]
                st.session_state.last_eval_plots_dir = out["plots_dir"]
                st.success("‚úÖ –ì–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É `report/`.")
            else:
                st.error(out.get("error", "Evaluation failed."))

    if st.session_state.last_eval_metrics:
        m = st.session_state.last_eval_metrics
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("Recall@K", f"{m['recall_at_k']:.3f}")
        c2.metric("MRR@K", f"{m['mrr_at_k']:.3f}")
        c3.metric("Avg Precision@K", f"{m['avg_precision_at_k']:.3f}")
        c4.metric("Mode", str(m.get("mode", "faiss")))

        st.caption(
            f"Embedding model: `{m.get('embed_model')}` | "
            f"min_score={m.get('min_score')} | keyword_filter={m.get('keyword_filter')}"
        )

        c5, c6, c7, c8 = st.columns(4)
        c5.metric("Hit@1", f"{m.get('hit_at_1', 0.0):.3f}")
        c6.metric("Hit@3", f"{m.get('hit_at_3', 0.0):.3f}")
        c7.metric("Hit@5", f"{m.get('hit_at_5', 0.0):.3f}")
        c8.metric("Top1 mean", f"{m.get('top1_score_mean'):.3f}" if m.get("top1_score_mean") is not None else "‚Äî")

    if isinstance(st.session_state.last_eval_df, pd.DataFrame):
        st.markdown("### üìã Evaluation table")
        st.dataframe(st.session_state.last_eval_df, use_container_width=True, hide_index=True)

    st.markdown("### üìà Plots")
    if st.session_state.last_eval_plots_dir:
        plots_dir = Path(st.session_state.last_eval_plots_dir)
        for p in ["hit_ratio.png", "hit_rank_hist.png", "top1_score_hist.png"]:
            fp = plots_dir / p
            if fp.exists():
                st.image(str(fp), caption=p)

    st.divider()
    st.markdown("### üì¶ Dataset statistics (local_cache.jsonl)")
    stats = dataset_stats_from_cache(Path(CONFIG.local_cache_path))
    if stats.get("ok"):
        df_types = pd.DataFrame([{"source_type": k, "chunks": v} for k, v in stats["types"].items()])
        st.dataframe(df_types, use_container_width=True, hide_index=True)
        st.bar_chart(df_types.set_index("source_type"))

    st.divider()
    st.markdown("### üì¶ Export for report (ZIP)")
    if st.button("üì• Export report package (ZIP)"):
        zp = export_report_zip()
        if not zp:
            st.error("–ù–µ–º–∞—î –ø–∞–ø–∫–∏ report/ –∞–±–æ —Ñ–∞–π–ª—ñ–≤. –°–ø–µ—Ä—à—É –∑–∞–ø—É—Å—Ç–∏ evaluation.")
        else:
            with open(zp, "rb") as f:
                st.download_button(
                    label="‚¨áÔ∏è Download report_package.zip",
                    data=f,
                    file_name="report_package.zip",
                    mime="application/zip",
                )

    st.divider()
    st.markdown("## üö® Bad queries panel (Feedback)")

    with st.expander("Bad queries", expanded=False):
        if not FEEDBACK_PATH.exists():
            st.info("–©–µ –Ω–µ–º–∞—î feedback.")
        else:
            rows = safe_read_jsonl(FEEDBACK_PATH, limit=5000)
            if not rows:
                st.info("Feedback —Ñ–∞–π–ª —î, –∞–ª–µ –Ω–µ –≤–¥–∞–ª–æ—Å—è –∑—á–∏—Ç–∞—Ç–∏ JSONL.")
            else:
                df_fb = pd.DataFrame(rows)
                st.dataframe(df_fb, use_container_width=True, hide_index=True)

                if "rating" in df_fb.columns:
                    bad_df = df_fb[df_fb["rating"] == -1]
                    st.caption(f"Bad queries: {len(bad_df)}")
                    st.dataframe(bad_df, use_container_width=True, hide_index=True)


# ==========================================================
# System status
# ==========================================================
st.divider()
st.subheader("üß™ –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º–∏")
st.write(
    {
        "online_mode": st.session_state.online_mode,
        "index_ready": st.session_state.index_ready,
        "local_cache_path": str(CONFIG.local_cache_path),
        "index_path": str(CONFIG.faiss_index_path),
        "meta_path": str(CONFIG.faiss_meta_path),
        "embed_model": CONFIG.embed_model_name,
        "llm_enabled": bool(st.session_state.llm_enabled),
        "llm_provider": st.session_state.llm_provider,
        "llm_model": st.session_state.llm_model,
        "reranker_available": bool(RERANK_AVAILABLE),
        "reranker_enabled": bool(st.session_state.use_reranker_ui and RERANK_AVAILABLE),
        "doc_scope_enabled": bool(st.session_state.doc_scope_enabled),
        "doc_scope_ids": sorted(list(st.session_state.doc_scope_ids or [])),
        "timestamp": datetime.now().isoformat(timespec="seconds"),
    }
)
