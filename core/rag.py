# core/rag.py
from __future__ import annotations

import re
from typing import List, Tuple, Optional, Dict

from .sources import SourceChunk
from .llm import LLMSettings, chat_completion


def _clean_text(t: str) -> str:
    t = (t or "").strip()
    t = t.replace("\xa0", " ")
    t = re.sub(r"\s+", " ", t)
    return t


def _snip(text: str, max_len: int = 320) -> str:
    text = _clean_text(text)
    if len(text) <= max_len:
        return text
    return text[:max_len].rstrip() + "‚Ä¶"


def _extract_short_summary(text: str, max_sentences: int = 2) -> str:
    t = _clean_text(text)
    if not t:
        return ""
    parts = re.split(r"(?<=[\.\!\?])\s+", t)
    parts = [p.strip() for p in parts if len(p.strip()) > 20]
    if not parts:
        return _snip(t, 220)
    return " ".join(parts[:max_sentences])


def _format_sources_md(retrieved: List[Tuple[SourceChunk, float]], top: int = 5) -> str:
    lines = []
    for i, (chunk, score) in enumerate(retrieved[:top], start=1):
        url = chunk.url or ""
        title = chunk.title or "–¥–∂–µ—Ä–µ–ª–æ"
        src_type = chunk.source_type
        lines.append(f"{i}. [{title}]({url}) ‚Äî `{src_type}` ‚Ä¢ score={score:.3f}")
    return "\n".join(lines)


def _build_context(retrieved: List[Tuple[SourceChunk, float]], max_chars: int = 6000) -> str:
    """
    Build compact RAG context from top chunks.
    """
    ctx_parts = []
    total = 0
    for i, (chunk, score) in enumerate(retrieved[:6], start=1):
        piece = f"[{i}] ({chunk.source_type}) {chunk.title}\nURL: {chunk.url}\nTEXT: {chunk.text}\n"
        piece = piece.strip() + "\n\n"
        if total + len(piece) > max_chars:
            break
        ctx_parts.append(piece)
        total += len(piece)
    return "".join(ctx_parts)


def make_answer_no_llm(query: str, retrieved: List[Tuple[SourceChunk, float]]) -> str:
    """
    Better offline MVP answer:
    - short summary from best chunk
    - evidence list
    """
    if not retrieved:
        return (
            f"**–ó–∞–ø–∏—Ç:** {query}\n\n"
            "‚ùå **–ù—ñ—á–æ–≥–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —É –ª–æ–∫–∞–ª—å–Ω—ñ–π –±–∞–∑—ñ.**\n\n"
            "–°–ø—Ä–æ–±—É–π —ñ–Ω—à–∏–π –∑–∞–ø–∏—Ç –∞–±–æ –Ω–∞—Ç–∏—Å–Ω–∏ **Sync knowledge base**, —â–æ–± –æ–Ω–æ–≤–∏—Ç–∏ –¥–∞–Ω—ñ."
        )

    best_text = retrieved[0][0].text

    lines = []
    lines.append(f"**–ó–∞–ø–∏—Ç:** {query}\n")
    lines.append("### ‚úÖ –í—ñ–¥–ø–æ–≤—ñ–¥—å (offline / –±–µ–∑ LLM)")
    lines.append(_extract_short_summary(best_text, max_sentences=2))
    lines.append("\n---\n")
    lines.append("### üìå –î–∂–µ—Ä–µ–ª–∞")
    lines.append(_format_sources_md(retrieved, top=5))
    return "\n".join(lines)


def make_answer_with_llm(
    query: str,
    retrieved: List[Tuple[SourceChunk, float]],
    llm: LLMSettings,
) -> str:
    """
    RAG answer with LLM:
    - use retrieved chunks as context
    - ask model to answer only from provided sources
    - include sources block
    """
    if not retrieved:
        return make_answer_no_llm(query, retrieved)

    context = _build_context(retrieved)

    system = (
        "–¢–∏ ‚Äî –ø–æ–º—ñ—á–Ω–∏–∫ –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç—ñ–≤ –Ü–ö–ù–Ü –õ–ü–ù–£. "
        "–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é. "
        "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –Ω–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É. "
        "–Ø–∫—â–æ —É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ –Ω–µ–º–∞—î –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ ‚Äî —á–µ—Å–Ω–æ —Å–∫–∞–∂–∏, —â–æ –¥–∞–Ω–∏—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ. "
        "–í—ñ–¥–ø–æ–≤—ñ–¥—å —Ä–æ–±–∏ –∫–æ—Ä–æ—Ç–∫–æ —ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–æ (1‚Äì6 –ø—É–Ω–∫—Ç—ñ–≤), –±–µ–∑ –≤–æ–¥–∏."
    )

    user = (
        f"–ü–∏—Ç–∞–Ω–Ω—è: {query}\n\n"
        f"–ö–æ–Ω—Ç–µ–∫—Å—Ç (–¥–∂–µ—Ä–µ–ª–∞):\n{context}\n\n"
        "–ó–≥–µ–Ω–µ—Ä—É–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å. –ù–∞–ø—Ä–∏–∫—ñ–Ω—Ü—ñ –¥–æ–¥–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π –±–ª–æ–∫ '–î–∂–µ—Ä–µ–ª–∞:' "
        "—ñ –ø–µ—Ä–µ—Ä–∞—Ö—É–π –Ω–æ–º–µ—Ä–∏ [1], [2]... —è–∫—ñ —Ç–∏ —Ä–µ–∞–ª—å–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–≤."
    )

    try:
        content = chat_completion(
            llm,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
        )
    except Exception as e:
        # If LLM failed, fallback to offline
        return (
            f"‚ö†Ô∏è **LLM –ø–æ–º–∏–ª–∫–∞:** `{e}`\n\n"
            + make_answer_no_llm(query, retrieved)
        )

    # Add real clickable sources (from retrieval)
    sources_md = _format_sources_md(retrieved, top=5)

    return (
        f"**–ó–∞–ø–∏—Ç:** {query}\n\n"
        f"### ‚úÖ –í—ñ–¥–ø–æ–≤—ñ–¥—å (LLM)\n"
        f"{content.strip()}\n\n"
        f"---\n"
        f"### üìå –î–∂–µ—Ä–µ–ª–∞ (retrieval —Ç–æ–ø-5)\n"
        f"{sources_md}"
    )
